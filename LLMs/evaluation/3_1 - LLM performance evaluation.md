---
date modified: Tuesday, January 20th 2026, 2:35:27 pm
---
```table-of-contents
```

## Easy guides
- take a look at - [[3_0 - LLM intro evaluation]]

Classical NLP usecases:
1. **Summarization (and paraphrasing)**
2. **Question answering**
3. **Instruction following**
4. **named entity recognition**
5. **relation extraction**
6. **information extraction**
7. **classification**
8. **grammar & syntax**

LLM usecases: 
1. **structured output**
2. **Long-context processing** 
    - Long-document understanding
3. **Multimodal understanding** -
    - Visual question answering
    - Image understanding
4. **Function calling / Tool use** 
5. **embeddings**
6. **Maths**
7. **Knowledge  & grounding** 
	- world knowledge
	- domain expertise
    - Fact verification
8. **Creative generation** 
    - Story writing
    - Dialogue generation
    - Poetry/artistic text
9. **efficiency & infrastructure** 
	- latency
	- cost
	- ttft
	- context size
	- CO2
10. **(later) Reasoning
11. **(later) Agent capabilities** 
12. **(Not included) Coding**

### Benchmarks and datasets
- Look at [[0- LLM Leaderboards and Datasets]]

### Metrics
#### Global metrics
- https://aman.ai/primers/ai/evaluation-metrics/#evaluation-metrics-for-generative-text-models
#### Hallucination (or factuality)

- [x] Hallucination Leaderboard ✅ 2025-02-07
	- blog : https://huggingface.co/blog/leaderboard-hallucinations
	- leaderboard : https://huggingface.co/spaces/hallucinations-leaderboard/leaderboard
- [x] [llm-hallucination-survey](https://github.com/HillZhang1999/llm-hallucination-survey#llm-hallucination-survey) : ✅ 2025-02-07
	- Evaluation format : the ability to generate factual statements or to discriminate them from non-factual ones.
		-  **Generation Benchmarks:** These benchmarks, such as TruthfulQA and FactScore, evaluate the quality of text generated by LLMs by directly assessing the factual accuracy of generated text.
		- **Discrimination Benchmarks:** These benchmarks, such as HaluEval and FACTOR, assess the LLM's ability to distinguish between factual and non-factual statements.
	- Task format : 
		- **Question Answering (QA):** Evaluating truthfulness in answers to knowledge-intensive questions.
		- **Task Instructions (TI):** Evaluating factuality in responses prompted by instructions.
		- **Text Completion (TC):** Identifying hallucinations that appear during generation of informative statements.
- [x] [awesome-hallucination-detection](https://github.com/EdinburghNLP/awesome-hallucination-detection) : Everything + metrics for each paper ✅ 2025-02-07
- [x] [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://arxiv.org/abs/2305.14251) ✅ 2025-02-07

#### Perplexity & Shannon: 
- [ ] https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4
- [x] https://medium.com/@anudevmanjusatheesh/perplexity-unveiled-key-metric-for-evaluating-llm-output-6ffccf5d0d85 ✅ 2025-03
- [ ] https://www.comet.com/site/blog/perplexity-for-llm-evaluation/
#### RAG Evaluation

RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations.

- Awesome list of rag eval tools: https://github.com/YHPeter/Awesome-RAG-Evaluation

- Recommanded tools
	- RAGAS : 
		- GitHub : https://github.com/explodinggradients/ragas
		- Paper : https://arxiv.org/abs/2309.15217


General metrics
- Fastest
- Latency (TTFT)
- Context size

Context dependent metrics : 
- RAG 
- summarization
- translation

**LLM General Metrics**

| Metric                                                                                                                                    | Stage              | Description                                                                                                                                        |
| ----------------------------------------------------------------------------------------------------------------------------------------- | ------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| Cosine Similarity                                                                                                                         | Embeddings         | Measures the similarity between query and document embeddings                                                                                      |
| Perplexity                                                                                                                                | Generation         | Evaluates how well the model predicts a sample, lower is better                                                                                    |
| BLEU Score                                                                                                                                | Generation         | Measures the similarity between generated text and reference text. Evaluating machine translation tasks, where precision and fluency are critical. |
| ROUGE Score                                                                                                                               | Generation         | Evaluates the quality of generated summaries. for summarization tasks where capturing key ideas and recall is more important than exact wording.   |
| METEOR Score                                                                                                                              | Generation         | Assesses the quality of machine translation or text generation                                                                                     |
| BERTScore                                                                                                                                 | Generation         | Computes the similarity of two sentences using contextual embeddings                                                                               |
| MoverScore                                                                                                                                | Generation         |                                                                                                                                                    |
| Faithfulness / Hallucination Rate                                                                                                         | Generation         | Measures how accurately the generated text reflects the retrieved information                                                                      |
| Relevance                                                                                                                                 | End-to-end         | Assesses how relevant the generated response is to the input query                                                                                 |
| Coherence                                                                                                                                 | End-to-end         | Evaluates the logical flow and consistency of the generated text                                                                                   |
| Fluency                                                                                                                                   | End-to-end         | Measures the grammatical correctness and naturalness of the generated text                                                                         |
| Exact-Match                                                                                                                               | Generation         |                                                                                                                                                    |
| Answer Correctness                                                                                                                        | End-to-end         | Evaluates the factual accuracy of the generated answers                                                                                            |
| Human Evaluation                                                                                                                          | End-to-end         | Subjective assessment of overall quality, relevance, and usefulness                                                                                |
| Inference Latency                                                                                                                         | System Performance | Measures the time taken to generate a response                                                                                                     |
| Throughput                                                                                                                                | System Performance | Measures tokens processed per second.                                                                                                              |
| Memory Utilization                                                                                                                        |                    | Tracks **GPU/CPU memory consumption** during inference and training.                                                                               |
| Cost per Query                                                                                                                            |                    | Estimates **operational cost per API call**                                                                                                        |
| Energy Efficiency                                                                                                                         |                    | Measures **power consumption during inference**.                                                                                                   |
|                                                                                                                                           |                    |                                                                                                                                                    |
| [Tone Critique](https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/language_features/tone_critique.ipynb)<br>                |                    | Assess if the tone of machine-generated responses matches with the desired persona.                                                                |
| [Language Critique](https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/language_features/language_critique.ipynb)            |                    | Evaluate LLM generated responses on multiple aspects - fluence, politeness, grammar, and coherence.                                                |
| [Conversation Satisfaction](https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/conversation/conversation_satisfaction.ipynb) |                    | Measures the user’s satisfaction with the conversation with the AI assistant based on completeness and user acceptance.                            |
| [Guideline Adherence](https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/custom/guideline_adherence.ipynb)                   |                    | Grade how well the LLM adheres to a given custom guideline.                                                                                        |
|                                                                                                                                           |                    |                                                                                                                                                    |

**Information Extraction / RAG / Information Retrieval Metrics**

| Metric                                       | Stage                  | Description                                                                                                                                                                                                                                                           |
| -------------------------------------------- | ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Precision@k                                  | Retrieval (IR)         | Calculates the proportion of relevant documents in the top k results                                                                                                                                                                                                  |
| Recall@k                                     | Retrieval (IR)         | Measures the proportion of relevant documents retrieved in the top k results                                                                                                                                                                                          |
| F1 Score                                     | Retrieval (IR)         | Harmonic mean of precision and recall                                                                                                                                                                                                                                 |
| Mean Reciprocal Rank (MRR)                   | Retrieval (IR)         | Measures the average precision at different recall levels. Higher values indicate better performance.[source](https://www.pinecone.io/learn/offline-evaluation/#Mean-Reciprocal-Rank-(MRR))<br>Measures the rank of the first relevant document in the search results |
| Mean Average Precision (MAP)                 | Retrieval (IR)         | Evaluates the precision of retrieval at multiple recall levels[source](https://www.pinecone.io/learn/offline-evaluation/#Mean-Average-Precision-(MAP))                                                                                                                |
| Normalized Discounted Cumulative Gain (NDCG) | Retrieval (IR)         | Measures the quality of ranking, considering the position of relevant documents[source](https://www.pinecone.io/learn/offline-evaluation/#Normalized-Discounted-Cumulative-Gain-(NDCG@K))                                                                             |
| Precision                                    | Information Extraction | Measures the proportion of correctly extracted entities or information among all extracted items                                                                                                                                                                      |
| Recall                                       | Information Extraction | Evaluates the proportion of correctly extracted entities or information among all relevant items in the text                                                                                                                                                          |
| F1 Score                                     | Information Extraction | Harmonic mean of precision and recall, providing a single metric that balances both                                                                                                                                                                                   |
| Exact Match (EM)                             | Information Extraction | Measures the proportion of extractions that exactly match the ground truth                                                                                                                                                                                            |
| Entity-Level Precision                       | Information Extraction | Evaluates the accuracy of entity extraction, focusing on whether the correct entities are identified                                                                                                                                                                  |
| Entity-Level Recall                          | Information Extraction | Measures the ability to identify all relevant entities in the text                                                                                                                                                                                                    |
| Entity-Level F1 Score                        | Information Extraction | Combines entity-level precision and recall into a single metric                                                                                                                                                                                                       |
| Slot-Filling Accuracy                        | Information Extraction | Assesses how accurately the system fills in predefined slots with the correct information                                                                                                                                                                             |
| RAGAS Faithfulness                           | Generation             | Measures how factually consistent the generated answer is with the retrieved context                                                                                                                                                                                  |
| RAGAS Answer Relevancy                       | End-to-end             | Evaluates how relevant the generated answer is to the given question                                                                                                                                                                                                  |
| RAGAS Context Relevancy                      | Retrieval (IR)         | Assesses how relevant the retrieved context is to the given question                                                                                                                                                                                                  |
| RAGAS Context Precision                      | Retrieval (IR)         | Measures the proportion of relevant information in the retrieved context                                                                                                                                                                                              |
| RAGAS Context Recall                         | Retrieval (IR)         | Evaluates how much of the necessary information from the context is used in the answer                                                                                                                                                                                |
| RAGAS Harmfulness                            | End-to-end             | Detects potential harmful content in the generated answers                                                                                                                                                                                                            |


NER Metrics :

| Metric                     | Description                                                                                                |
|----------------------------|------------------------------------------------------------------------------------------------------------|
| **Precision**              | The proportion of correctly identified entities out of all identified entities.                            |
| **Recall**                 | The proportion of actual entities that were correctly identified.                                          |
| **F1 Score**               | The harmonic mean of precision and recall, balancing the two metrics.                                      |
| **Entity-Level Precision** | Precision calculated separately for each entity type (e.g., PERSON, ORGANIZATION).                         |
| **Entity-Level Recall**    | Recall calculated separately for each entity type.                                                         |
| **Entity-Level F1 Score**  | F1 Score calculated separately for each entity type.                                                       |
| **Token-Level Precision**  | Precision measured at the token level, assessing the accuracy of entity boundaries.                        |
| **Token-Level Recall**     | Recall measured at the token level, assessing the ability to capture all tokens that are part of entities. |
| **Token-Level F1 Score**   | F1 Score measured at the token level.                                                                      |
| **Exact Match**            | The proportion of entities with exact matches to ground truth annotations.                                 |
| **Partial Match**          | Measures how well the model identifies parts of entities that overlap with ground truth.                   |
| **Coverage**               | The proportion of ground truth entities that are captured by the model.                                    |
| **Precision at K (P@K)**   | Precision of the top K predictions.                                                                        |
| **Entity-Level Accuracy**  | The percentage of entities that are correctly identified with correct boundaries and labels.               |
| **Average Precision (AP)** | Average precision across different recall levels, summarizing precision performance.                       |
| **Average Recall (AR)**    | Average recall across different precision levels, summarizing recall performance.                          |
| **Span-Level Precision**   | Precision based on the spans of text identified as entities.                                               |
| **Span-Level Recall**      | Recall based on the spans of text identified as entities.                                                  |
| **Span-Level F1 Score**    | F1 Score based on the spans of text identified as entities.                                                |

## Tools / libraries 
- Deepeval
- promptfoo
- G-Eval
- Ragas

## Papers and research
-
## News and updates
- 
## Other resources 
- You need to understand the difference between recall and recall@K. if  K=N in Recall@K, it effectively becomes Recall.
- [RAGAS paper](https://arxiv.org/pdf/2309.15217) 
- [Survey RAG eval tools](https://arxiv.org/pdf/2405.07437)
- https://www.pinecone.io/learn/offline-evaluation/
- https://aman.ai/primers/ai/evaluation-metrics
- [Boolean vs Keyword/Lexical search vs Semantic](https://aarontay.medium.com/boolean-vs-keyword-lexical-search-vs-semantic-keeping-things-straight-95eb503b48f5)
- https://medium.com/@autorag/tips-to-understand-rag-generation-metrics-70dfcd988709


