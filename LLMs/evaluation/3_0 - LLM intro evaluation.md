---
date modified: Tuesday, December 2nd 2025, 10:27:31 am
---
```table-of-contents
```
## Easy guides
LLM evaluation is the process of testing and measuring how well large language models perform
- https://www.superannotate.com/blog/llm-evaluation-guide
- https://github.com/huggingface/evaluation-guidebook
- https://github.com/Hannibal046/Awesome-LLM
- https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation
### LLM as A Judge
- [[0- LLM Leaderboards#Judge / Evaluators]]
- [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)
		- https://arxiv.org/pdf/2411.15594
		- granit guard imb : https://arxiv.org/html/2412.07724v1#S6

## Tools / libraries 
- [[3_1 - LLM performance evaluation#Tools / libraries]]
- [[3_2 - LLM red-teaming evaluation#Tools / libraries]]
## Papers and research
-
## News and updates
-
## Other resources 
- Check my github stars /lists to find some resources



