---
date modified: Tuesday, January 20th 2026, 2:34:56 pm
---
```table-of-contents
```
## Easy guides
LLM evaluation is the process of testing and measuring how well large language models perform
- https://www.superannotate.com/blog/llm-evaluation-guide
- https://github.com/huggingface/evaluation-guidebook
- https://github.com/Hannibal046/Awesome-LLM
- https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation
- https://www.confident-ai.com/blog/g-eval-the-definitive-guide

### Two Categories of Metrics

#### **1. Answer Correctness Metrics** (evaluation metrics)
These determine **whether an answer is correct or not** - they compare predictions against ground truth.

**Examples:**
- Exact Match
- Semantic Accuracy
- Type-Aware Accuracy
- LLM-as-Judge
- BERTScore
- BLEU
- Numerical Accuracy with Tolerance

**Purpose:** Binary or graded assessment of correctness
**other naming for this category**: Evaluation Methods, Scoring methods

---
### **2. Performance Metrics** (Aggregate Statistics)
These measure **overall system performance** across a dataset using the results from correctness metrics.

**Examples:**
- Accuracy (% correct)
- Precision
- Recall
- F1 Score
- Macro/Micro averages

**Purpose:** Summarize performance across multiple examples


### Benchmarks and datasets
- Look at [[0- LLM Leaderboards and Datasets]]
### LLM as A Judge
- [[0- LLM Leaderboards#Judge / Evaluators]]
- [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)
		- https://arxiv.org/pdf/2411.15594
		- granit guard IBM : https://arxiv.org/html/2412.07724v1#S6

## Tools / libraries 
- [[3_1 - LLM performance evaluation#Tools / libraries]]
- [[3_2 - LLM safety & red-teaming evaluation#Tools / libraries]]
- 
## Papers and research
-
## News and updates
-
## Other resources 
- Check my github stars /lists to find some resources



