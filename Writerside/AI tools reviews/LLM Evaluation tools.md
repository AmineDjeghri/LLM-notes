
| Tool name | Description | Hosting | Benchmark | Evaluation | Redteam | monitoring | CLI | UI  | runtime |
| --------- | ----------- | ------- | --------- | ---------- | ------- | ---------- | --- | --- | ------- |
| Promptfoo |             | Local   |           |            | ✅       |            |     |     | nvm     |
| Langsmith |             |         |           |            |         |            |     |     |         |
| Giskard   |             |         |           |            |         |            |     |     | pip     |
| DeepEval  |             |         |           |            | ✅       |            |     |     | pip     |
|           |             |         |           |            |         |            |     |     |         |
|           |             |         |           |            |         |            |     |     |         |

- Features
- Maturity
- Data location
- Easy to use
- Licensing
Metrics

langsmith : 
	- test  
	- est ce que les données sont secure meme si on utilise nos modèles
	- est ce que les données sont dans un tenant

- Giskard feature 
- comparer les features
- Giskard (free vs paid)
- utiliser Giskard avec openai (emulation openai)
- Giskard vs Promptfoo (free vs free)


dspy

Every tool has this fields of comparaison : 
- Package :
	- description
	- site/github
	- docs
	- price 
- Web platform :
	- description
	- site/github 
	- docs:
	- price : free, freemium, paid - pricing page 
	- local, cloud
	- 
## Dashboard & reporting
### Promptfoo

### Giskard (by Giskard-AI)

### LangSmith (by LangChain)

### DeepEval (by ConfidentAI)
 - Package :
	- <u>Description</u> : 
		- **DeepEval** is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs **locally on your machine** for evaluation. 
		- Whether your application is implemented via RAG or fine-tuning, LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal hyperparameters to improve your RAG pipeline, prevent prompt drifting, or even transition from OpenAI to hosting your own Llama2 with confidence.
	- <u>Github</u> : https://github.com/confident-ai/deepeval
	- <u>Docs</u> : https://docs.confident-ai.com/docs/getting-started
	- python package, pip installable
	- features :
		- Easily "unit test" LLM outputs in a similar way to Pytest.
		- Plug-and-use 14+ LLM-evaluated metrics, most with research backing.
		- Synthetic dataset generation with state-of-the-art evolution techniques.
		- Metrics are simple to customize and covers all use cases.
		- Real-time evaluations in production.
		- integrates natively with [Confident AI](https://app.confident-ai.com/)
- Web platform :
	- Cloud only
	- **evaluate, regression test, and monitor** LLM applications on the cloud.
	- site : https://www.confident-ai.com/
	- docs : https://docs.confident-ai.com/docs/confident-ai-introduction
	- paid, only 7 days trial , pricing : https://www.confident-ai.com/pricing
	- 
	
### Prompt-flow Tracing (by Azure)
### Langfuse
### Validate (by Tonic)
### AutoRAG


## Evaluation benchmarks





